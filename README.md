# ğŸ­ TalkMateAI

**Real-time Voice-Controlled 3D Avatar with Multimodal AI**

> Your 3D AI companion that never stops listening, never stops caring. 

> Transform conversations into immersive experiences with AI-powered 3D avatars that see, hear, and respond naturally.

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-15+-black.svg)](https://nextjs.org)
[![CUDA](https://img.shields.io/badge/CUDA-12.4-76B900.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)


## ğŸ¥ Demo Video

[![TalkMateAI Demo](https://img.youtube.com/vi/dE_8TXmp2Sk/maxresdefault.jpg)](https://www.youtube.com/watch?v=dE_8TXmp2Sk)

## âœ¨ Features

### ğŸ¯ **Core Capabilities**
- **ğŸ¤ Real-time Voice Activity Detection** - Advanced VAD with configurable sensitivity
- **ğŸ—£ï¸ Speech-to-Text** - Powered by OpenAI Whisper (tiny model) for instant transcription
- **ğŸ‘ï¸ Vision Understanding** - SmolVLM2-256M-Video-Instruct for multimodal comprehension
- **ğŸ”Š Natural Text-to-Speech** - Kokoro TTS with native word-level timing
- **ğŸ­ 3D Avatar Animation** - Lip-sync and emotion-driven animations using [TalkingHead](https://github.com/met4citizen/TalkingHead)

### ğŸš€ **Advanced Features**
- **ğŸ“¹ Camera Integration** - Real-time image capture with voice commands
- **âš¡ Streaming Responses** - Chunked audio generation for minimal latency
- **ğŸ¬ Native Timing Sync** - Perfect lip-sync using Kokoro's native timing data
- **ğŸ¨ Draggable Camera View** - Floating, resizable camera interface
- **ğŸ“Š Real-time Analytics** - Voice energy visualization and transmission tracking
- **ğŸ”„ WebSocket Communication** - Low-latency bidirectional data flow

## ğŸ—ï¸ Architecture
![System Architecture](./images/architecture.svg)


## ğŸ› ï¸ Technology Stack

### Backend (Python)
- **ğŸ§  AI Models from HuggingFaceğŸ¤—:**
  - `openai/whisper-tiny` - Speech recognition
  - `HuggingFaceTB/SmolVLM2-256M-Video-Instruct` - Vision-language understanding
  - `Kokoro TTS` - High-quality voice synthesis
- **âš¡ Framework:** FastAPI with WebSocket support
- **ğŸ”§ Processing:** PyTorch, Transformers, Flash Attention 2
- **ğŸµ Audio:** SoundFile, NumPy for real-time processing

### Frontend (TypeScript/React)
- **ğŸ–¼ï¸ Framework:** Next.js 15 with TypeScript
- **ğŸ¨ UI:** Tailwind CSS + shadcn/ui components
- **ğŸ­ 3D Rendering:** [TalkingHead](https://github.com/met4citizen/TalkingHead) library
- **ğŸ™ï¸ Audio:** Web Audio API with AudioWorklet
- **ğŸ“¡ Communication:** Native WebSocket with React Context

### ğŸ”§ **Development Tools**
- **ğŸ“¦ Package Management:** UV (Python) + PNPM (Node.js)
- **ğŸ¨ Code Formatting:** 
  - **Backend:** Black (Python)
  - **Frontend:** Prettier (TypeScript/React)
- **ğŸ” Quality Control:** Husky for pre-commit hooks

## ğŸ“‹ Requirements

### System Tested on
- **OS:** Windows 11 (Linux/macOS support coming soon, will create a docker image)
- **GPU:** NVIDIA RTX 3070 (8GB VRAM)

## ğŸš€ Quick Start

### 1. Prerequisites
- Node.js 20+
- PNPM
- Python 3.10
- UV (Python package manager)


### 2. **Setup monorepo dependencies from root**
```bash
# will setup both frontend and backend but require the prerequisites
pnpm run monorepo-setup
```

### 3. **Development Workflow**
```bash
# Format code before committing (recommended)
pnpm format
```

### 4. Run the Application

 **Start Development Servers**
```bash
# Run both frontend and backend from root
pnpm dev

# Or run individually
pnpm dev:client  # Frontend (http://localhost:3000)
pnpm dev:server  # Backend (http://localhost:8000)
```

### 5. Initial Setup
1. **Allow microphone access** when prompted
2. **Enable camera** for multimodal interactions
3. **Click "Connect"** to establish WebSocket connection
4. **Start Voice Control** and begin speaking!

## ğŸ® Usage Guide

### Camera Controls
- **Drag** to move camera window
- **Resize** with maximize/minimize buttons
- **Toggle on/off** as needed

### Voice Settings
- **Energy Threshold:** Adjust sensitivity to background noise
- **Pause Duration:** How long to wait before processing speech
- **Min/Max Speech:** Control segment length limits


## ğŸ™ Acknowledgments

- **TalkingHead** ([met4citizen](https://github.com/met4citizen/TalkingHead)) for 3D avatar rendering and lip-sync
- **yeyu2** ([Multimodal-local-phi4](https://github.com/yeyu2/Youtube_demos/tree/main/Multimodal-local-phi4)) for multimodal implementation inspiration



---

<div align="center">

**â­ Star this repo if you find it useful! â­**

Made with â¤ï¸ by the Kiranbaby14

</div>
